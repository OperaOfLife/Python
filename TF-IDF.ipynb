{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d12b0c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[48, 10, 28, 37, 22, 24, 1, 39, 2, 36]\n",
      "1\n",
      "The aim is to defeat five rows of eleven aliens—although some versions feature different numbers—that move horizontally back and forth across the screen as they advance toward the bottom of the screen.\n",
      "\n",
      "2\n",
      "The player's laser cannon is partially protected by several stationary defense bunkers—the number also varies by version—that are gradually destroyed from the top and bottom by blasts from either the aliens or the player.\n",
      "\n",
      "10\n",
      "The game's inspiration is reported to have come from varying sources, including an adaptation of the mechanical game Space Monsters released by Taito in 1972, and a dream about Japanese school children who are waiting for Santa Claus when they are attacked by invading aliens.\n",
      "\n",
      "22\n",
      "Because microcomputers in Japan were not powerful enough at the time to perform the complex tasks involved in designing and programming Space Invaders, Nishikado had to design his own custom hardware and development tools for the game.\n",
      "\n",
      "24\n",
      "The game uses an Intel 8080 central processing unit (CPU), displays raster graphics on a CRT monitor using a bitmapped framebuffer, and uses monaural sound hosted by a combination of analog circuitry and a Texas Instruments SN76477 sound chip.\n",
      "\n",
      "28\n",
      "Despite the specially developed hardware, Nishikado was unable to program the game as he wanted—the Control Program board was not powerful enough to display the graphics in color or move the enemies faster—and he ended up considering the development of the game's hardware the most difficult part of the whole process.\n",
      "\n",
      "36\n",
      "The cabinet artwork featured large humanoid monsters not present in the game; Nishikado attributes this to the artist basing the designs on the original title of \"Space Monsters\", rather than referring to the actual in-game graphics.\n",
      "\n",
      "37\n",
      "In the upright cabinets, the game graphics are generated on a hidden CRT monitor and reflected toward the player using a semi-transparent mirror, behind which is mounted a plastic cutout of a moon bolted against a painted starry background.\n",
      "\n",
      "39\n",
      "Both Taito's and Midway's first Space Invaders versions had black-and-white graphics with a transparent colored overlay using strips of orange and green cellophane over certain portions of the screen to add color to the image.\n",
      "\n",
      "48\n",
      "It was also the first game where players were given multiple lives, had to repel hordes of enemies, could take cover from enemy fire, and use destructible barriers,in addition to being the first game to use a continuous background soundtrack, with four simple diatonic descending bass notes repeating in a loop, which was dynamic and changed pace during stages, like a heartbeat sound that increases pace as enemies approached.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "file = open('space_invaders.txt', encoding='utf-8')\n",
    "doc = file.read()\n",
    "file.close()\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "porter = nltk.stem.PorterStemmer()\n",
    "sentences = nltk.tokenize.sent_tokenize(doc)\n",
    "\n",
    "# treat each sentence as a document\n",
    "docs = []\n",
    "\n",
    "punc = str.maketrans('','', string.punctuation)\n",
    "for sent in sentences:\n",
    "    sent_no_punc = sent.translate(punc)\n",
    "    words_stemmed = [porter.stem(w) for w in sent_no_punc.lower().split()]\n",
    "    docs += [' '.join(words_stemmed)]    \n",
    "\n",
    "# for i in range(0, len(docs)):\n",
    "#     print('doc' + str(i) + '=', docs[i])\n",
    "\n",
    "tfidf_vec = TfidfVectorizer(analyzer='word')\n",
    "tfidf_wm = tfidf_vec.fit_transform(docs).toarray()\n",
    "# print(tfidf_wm)\n",
    "\n",
    "df_index = ['doc'+str(i) for i in range(len(docs))]\n",
    "# print(df_index)\n",
    "\n",
    "df_columns = tfidf_vec.get_feature_names()\n",
    "# print(df_columns)\n",
    "\n",
    "tfidf_df = pd.DataFrame(data=tfidf_wm, index=df_index, columns=df_columns)\n",
    "tfidf_sum_by_docs = np.sum(tfidf_df, axis=1)\n",
    "\n",
    "sorted_series = tfidf_sum_by_docs.sort_values(ascending=False)\n",
    "top_series = sorted_series.head(10)\n",
    "print(\"\\n\")\n",
    "\n",
    "# remove the prefix 'doc' from indexes such as\n",
    "# [doc10, doc24, doc11, ..., doc22]\n",
    "num_only = [int(x[3:]) for x in top_series.index]\n",
    "print(num_only)\n",
    "\n",
    "for i in sorted(num_only):\n",
    "\tprint(i)\n",
    "\tprint(sentences[i] + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e167038c",
   "metadata": {},
   "source": [
    "# Lecture Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81af14e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Anisha\n",
      "[nltk_data]     Sinha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Anisha\n",
      "[nltk_data]     Sinha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      big  cat  eat  fish  john\n",
      "doc1    0    1    0     0     1\n",
      "doc2    0    2    1     1     0\n",
      "doc3    1    0    1     1     0\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nltk.download('stopwords') # remove it in your system\n",
    "nltk.download('wordnet') # ditto above\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = stopwords.words ('english')\n",
    "\n",
    "docs= ['John has some cats.','Cats, being cats, eat fish.','I ate a big fish.']\n",
    "\n",
    "# data cleansing\n",
    "docs_clean = []\n",
    "punc = str.maketrans('','', string.punctuation)\n",
    "for doc in docs:\n",
    "    doc_no_punc = doc.translate(punc)\n",
    "    words = doc_no_punc. lower().split()\n",
    "    words = [lemmatizer.lemmatize (word, 'v')\n",
    "            for word in words if word not in stop_words]\n",
    "    docs_clean.append(' '.join(words))\n",
    "\n",
    "# generate feature vectors using BOW\n",
    "bow = CountVectorizer()\n",
    "feature_vectors = bow.fit_transform(docs_clean).toarray()\n",
    "vocab = bow.get_feature_names()\n",
    "\n",
    "df = pd.DataFrame(data=feature_vectors,index=['doc1', 'doc2', 'doc3'],columns=vocab)\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05521623",
   "metadata": {},
   "source": [
    "# Lecture Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b767633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           big       cat       eat      fish      john\n",
      "doc1  0.000000  0.605349  0.000000  0.000000  0.795961\n",
      "doc2  0.000000  0.816497  0.408248  0.408248  0.000000\n",
      "doc3  0.680919  0.000000  0.517856  0.517856  0.000000\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#nltk.download('stopwords') # remove it in your system\n",
    "#nltk.download('wordnet') # ditto above\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = stopwords.words ('english')\n",
    "\n",
    "docs= ['John has some cats.','Cats, being cats, eat fish.','I ate a big fish.']\n",
    "\n",
    "# data cleansing\n",
    "docs_clean = []\n",
    "punc = str.maketrans('','', string.punctuation)\n",
    "for doc in docs:\n",
    "    doc_no_punc = doc.translate(punc)\n",
    "    words = doc_no_punc. lower().split()\n",
    "    words = [lemmatizer.lemmatize (word, 'v')\n",
    "            for word in words if word not in stop_words]\n",
    "    docs_clean.append(' '.join(words))\n",
    "\n",
    "# generate feature vectors using BOW\n",
    "tfidf = TfidfVectorizer()\n",
    "feature_vectors = tfidf.fit_transform(docs_clean).toarray()\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data=feature_vectors,index=['doc1', 'doc2', 'doc3'],columns=tfidf.get_feature_names())\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1685c4c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
